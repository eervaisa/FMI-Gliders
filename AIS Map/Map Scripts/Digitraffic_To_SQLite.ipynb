{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from math import floor\n",
    "from sqlite3 import Error\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "# NOTE: Required downloads: \n",
    "# UpdatedPub150.csv from https://msi.nga.mil/Publications/WPI \n",
    "# code-list_csv.csv from https://datahub.io/core/un-locode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'Digitraffic-User': 'Foo/Bar'} # To track errors/loads by Digitraffic as requested in their guidelines\n",
    "\n",
    "# /locations: 'Find latest vessel locations by mmsi and optional timestamp interval in milliseconds from Unix epoch.'\n",
    "#   mmsi\n",
    "#   from\n",
    "#   to\n",
    "#   radius\n",
    "#   latitude\n",
    "#   longitude\n",
    "# /vessels: 'Return latest vessel metadata for all known vessels.'\n",
    "#   from\n",
    "#   to\n",
    "# /vessels/mmsi: 'Return latest vessel metadata by mmsi.'\n",
    "\n",
    "\n",
    "# NOTE: Python timestamps in seconds, digitraffic in milliseconds\n",
    "\n",
    "def get_ships_meta(since):\n",
    "    '''Get the metadata of all ships since given timestamp'''\n",
    "    # https://meri.digitraffic.fi/api/ais/v1/vessels\n",
    "    \n",
    "    host = \"https://meri.digitraffic.fi/api/ais/v1/\"\n",
    "    tag = \"vessels\"\n",
    "    shipmeta_url = f'{host}{tag}?from={since}'\n",
    "    meta_response = requests.get(shipmeta_url, headers=HEADERS)\n",
    "    return meta_response.json()\n",
    "\n",
    "def get_ship_meta(MMSI):\n",
    "    '''Get the metadata of a ship with MMSI'''\n",
    "    # https://meri.digitraffic.fi/api/ais/v1/vessels/338926878\n",
    "\n",
    "    host = \"https://meri.digitraffic.fi/api/ais/v1/\"\n",
    "    tag = \"vessels/\"\n",
    "    shipmeta_url = f'{host}{tag}{MMSI}'\n",
    "    meta_response = requests.get(shipmeta_url, headers=HEADERS)\n",
    "    return meta_response.json()\n",
    "    \n",
    "def get_ships_locations(latitude, longitude, distance, since):\n",
    "    '''Get the location of ships close to lat/lon'''\n",
    "    # https://meri.digitraffic.fi/api/ais/v1/locations?from=1692184014&radius=20&latitude=59.837&longitude=23.29\n",
    "    \n",
    "    host = \"https://meri.digitraffic.fi/api/ais/v1/\"\n",
    "    tag = \"locations\"\n",
    "    shiplocat_url = f'{host}{tag}?from={since}&radius={distance}&latitude={latitude}&longitude={longitude}'\n",
    "    locat_response = requests.get(shiplocat_url, headers=HEADERS)\n",
    "    return locat_response.json()\n",
    "\n",
    "def get_ship_location(mmsi):\n",
    "    '''Get the location of a ship with MMSI'''\n",
    "    # https://meri.digitraffic.fi/api/ais/v1/locations?mmsi=338926878\n",
    "    \n",
    "    host = \"https://meri.digitraffic.fi/api/ais/v1/\"\n",
    "    tag = \"locations\"\n",
    "    shiplocat_url = f'{host}{tag}?mmsi={mmsi}'\n",
    "    locat_response = requests.get(shiplocat_url, headers=HEADERS)\n",
    "    return locat_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destination analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extensive style edits\n",
    "\n",
    "def format_port_identifiers(locodes):\n",
    "    '''Format port names and locodes for regex purposes'''\n",
    "    # Edit names to help matching\n",
    "    locodes[\"SimpleName\"] = locodes[\"NameWoDiacritics\"].str.upper()\n",
    "    locodes.replace({\"SimpleName\": {r\"[^A-Z/(\\s]\": \"\"}}, regex = True, inplace = True)\n",
    "    locodes.replace({\"SimpleName\": {r\"\\s+\": \" \"}}, regex = True, inplace = True)\n",
    "    locodes.SimpleName.str.strip()\n",
    "\n",
    "    # Turn names/locodes into regex patterns\n",
    "    locodes[\"LocodePattern\"] = locodes[\"Country\"] + r\"\\s*\" + locodes[\"Location\"]\n",
    "    locodes[\"NamePattern\"] = locodes[\"SimpleName\"]\n",
    "    locodes.replace({\"NamePattern\": {r\"\\s*\\(\": \"|\"}}, regex = True, inplace = True)\n",
    "    locodes.replace({\"NamePattern\": {r\"/\": \"|\"}}, regex = True, inplace = True)\n",
    "\n",
    "    # Add exceptions for common alternatives\n",
    "    locodes.replace({\"NamePattern\": {\"SAINT PETERSBURG\": \"PETERSBURG|PETERBURG|SPB\"}}, inplace = True)\n",
    "    locodes.replace({\"NamePattern\": {\"USTLUGA\": \"UST[^A-Z]*LUGA\"}}, inplace = True) # \"UST'-LUGA\"\n",
    "    locodes.replace({\"NamePattern\": {\"TALLINN\": \"TALLINN|TALLIN\"}}, inplace = True)\n",
    "    locodes.replace({\"NamePattern\": {\"ANTWERPEN\": \"ANTWERPEN|ANTWERP\"}}, inplace = True)\n",
    "    locodes.replace({\"NamePattern\": {\"TURKU\": \"TURKU|PANSIO\"}}, regex = True, inplace = True)\n",
    "    # TODO:\n",
    "    # NAUVO    NAUVO|PROSTVIK|PARNAS    Nagu (Nauvo)\n",
    "    # PARAINEN LILLMALO|PARAINEN        Parainen (Pargas)\n",
    "    # HELSINKI HKI|HEL|HELSINKI         Helsingfors (Helsinki)\n",
    "    # KORPO    KORPO|RETAIS             Korpo (Korppoo)\n",
    "    # VYSOTSK  VISOTSK|VYSOTSK\n",
    "    # e.g. savonlinna -> saimaa|savonlinna\n",
    "    # retais-pärnäs Nauvo-Korppoo \n",
    "\n",
    "    # Edit names to help matching\n",
    "    locodes.replace({\"SimpleName\": {r\"\\(.*\": \"\"}}, regex = True, inplace = True)\n",
    "    locodes.replace({\"SimpleName\": {r\"/.*\": \"\"}}, regex = True, inplace = True)\n",
    "    locodes[\"SimpleName\"] = locodes.SimpleName.str.strip()\n",
    "\n",
    "    # Remove SimpleName value from duplicates outside Europe\n",
    "    # and all but one european\n",
    "    dupes = locodes[locodes.duplicated(subset = [\"SimpleName\"], keep = False)].copy()\n",
    "    eu_dupes = dupes[(dupes['Latitude'] > 25) & (dupes['Latitude'] < 75) & (dupes['Longitude'] > -25) & (dupes['Longitude'] < 45)]\n",
    "    eu_dupes = eu_dupes.drop_duplicates(subset = ['SimpleName'])\n",
    "    dupes[\"SimpleName\"] = \"\"\n",
    "    dupes.update(eu_dupes[\"SimpleName\"])\n",
    "    locodes.update(dupes[\"SimpleName\"], overwrite=True)\n",
    "    locodes.replace({\"SimpleName\": {r\"^\\s*$\": pd.NA}}, regex = True, inplace = True)\n",
    "\n",
    "    return locodes\n",
    "\n",
    "def convert_coordinates_ddmm_to_dddd(coord):\n",
    "    '''Convert coordinates from degrees+minutes to degrees+decimals'''\n",
    "    \n",
    "    degs = coord//100\n",
    "    mins = coord % 100\n",
    "\n",
    "    return round(degs + mins/60, 5)\n",
    "\n",
    "def extract_coordinates(locodes):\n",
    "    '''Split coordinates to floats from string'''\n",
    "    locodes[\"Latitude\"]  = locodes[\"Coordinates\"].str.extract(r\"(\\d*)[NS]\")\n",
    "    locodes[\"Longitude\"] = locodes[\"Coordinates\"].str.extract(r\"(\\d*)[WE]\")\n",
    "\n",
    "    locodes[\"Latitude\"]  = convert_coordinates_ddmm_to_dddd(pd.to_numeric(locodes[\"Latitude\"]))\n",
    "    locodes[\"Longitude\"] = convert_coordinates_ddmm_to_dddd(pd.to_numeric(locodes[\"Longitude\"]))\n",
    "\n",
    "    locodes.loc[locodes[\"Coordinates\"].str.contains('S', na = False), \"Latitude\"] *= -1\n",
    "    locodes.loc[locodes[\"Coordinates\"].str.contains('W', na = False), \"Longitude\"] *= -1 \n",
    "\n",
    "    return locodes\n",
    "\n",
    "def manual_coordinate_updates(locodes):\n",
    "    '''Add missing coordinates'''\n",
    "    # TODO: Check all baltic ports with missing coordinates\n",
    "    # TODO: Manually add missing data\n",
    "    locodes.loc[(locodes[\"Country\"] == \"RU\") & (locodes[\"Location\"] == \"IAR\"), \"Coordinates\"] = \"5760N 03991E\" # Yaroslavl\n",
    "    locodes.loc[(locodes[\"Country\"] == \"RU\") & (locodes[\"Location\"] == \"LOM\"), \"Coordinates\"] = \"5993N 03033E\" # Lomonosov\n",
    "    locodes.loc[(locodes[\"Country\"] == \"SE\") & (locodes[\"Location\"] == \"ROR\"), \"Coordinates\"] = \"5993N 02438E\" # Rönnskär\n",
    "    locodes.loc[(locodes[\"Country\"] == \"RU\") & (locodes[\"Location\"] == \"ONG\"), \"Coordinates\"] = \"6391N 03809E\" # Onega\n",
    "    locodes.loc[(locodes[\"Country\"] == \"FI\") & (locodes[\"Location\"] == \"PUU\"), \"Coordinates\"] = \"6152N 02817E\" # Puumala\n",
    "    locodes.loc[(locodes[\"Country\"] == \"FI\") & (locodes[\"Location\"] == \"HAU\"), \"Coordinates\"] = \"6518N 02532E\" # Haukipudas\n",
    "    locodes.loc[(locodes[\"Country\"] == \"SE\") & (locodes[\"Location\"] == \"HLD\"), \"Coordinates\"] = \"6368N 02034E\" # Holmsund\n",
    "    locodes.loc[(locodes[\"Country\"] == \"FI\") & (locodes[\"Location\"] == \"VKO\"), \"Coordinates\"] = \"6041N 02626E\" # Valkom\n",
    "    locodes.loc[(locodes[\"Country\"] == \"SE\") & (locodes[\"Location\"] == \"BAT\"), \"Coordinates\"] = \"6579N 02342E\" # Båtskärsnäs\n",
    "    locodes.loc[(locodes[\"Country\"] == \"SE\") & (locodes[\"Location\"] == \"OGR\"), \"Coordinates\"] = \"6047N 01843E\" # Öregrund\n",
    "    locodes.loc[(locodes[\"Country\"] == \"GI\") & (locodes[\"Location\"] == \"GIB\"), \"Coordinates\"] = \"3609N 00520W\" # Gibraltar\n",
    "    locodes.loc[(locodes[\"Country\"] == \"IN\") & (locodes[\"Location\"] == \"KRI\"), \"Coordinates\"] = \"1424N 08013W\" # Krishnapatnam\n",
    "    locodes.loc[(locodes[\"Country\"] == \"FI\") & (locodes[\"Location\"] == \"LAN\"), \"Coordinates\"] = \"6007N 02018W\" # Långnäs\n",
    "    locodes.loc[(locodes[\"Country\"] == \"IT\") & (locodes[\"Location\"] == \"RAN\"), \"Coordinates\"] = \"4447N 01226W\" # Ravenna\n",
    "\n",
    "    return locodes\n",
    "\n",
    "def manual_port_additions(locodes):\n",
    "    '''Add completely new rows for frequently used missing locations'''\n",
    "    # TODO: \n",
    "    new_row = {'Country':'SE', 'Location':'SAH', 'Coordinates':'5609N 01585E', 'Name':'Sandhamn', 'NameWoDiacritics':'Sandhamn'}\n",
    "    locodes = pd.concat([locodes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    new_row = {'Country':'SE', 'Location':'NRR', 'Coordinates':'5893N 01797E', 'Name':'Norvik', 'NameWoDiacritics':'Norvik'}\n",
    "    locodes = pd.concat([locodes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    new_row = {'Country':'EE', 'Location':'KUN', 'Coordinates':'5951N 02656E', 'Name':'Kunda', 'NameWoDiacritics':'Kunda'}\n",
    "    locodes = pd.concat([locodes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    new_row = {'Country':'SE', 'Location':'GR2', 'Coordinates':'6034N 01846E', 'Name':'Gräsö', 'NameWoDiacritics':'Graso'}\n",
    "    locodes = pd.concat([locodes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    new_row = {'Country':'EE', 'Location':'MUU', 'Coordinates':'5950N 02495E', 'Name':'Muuga', 'NameWoDiacritics':pd.NA} # Some use this instead of EEMUG\n",
    "    locodes = pd.concat([locodes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # new_row = {'Country':'FI', 'Location':'OLU', 'Coordinates':'5951N 02656E', 'Name':'Oulu', 'NameWoDiacritics':'Oulu'} 2012: The code FIOLU also exist in UN/LOCODE but should not be used\n",
    "    # locodes = pd.concat([locodes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # Consider adding Utö, but take note SEUTO is separate and exists (though not in our data)\n",
    "    # FIUTO doesn't exist, we could just make it as a placeholder\n",
    "    # STIRSUDDEN used a lot, consider making it up (eg XX STR)\n",
    "\n",
    "    return locodes\n",
    "\n",
    "def world_port_index_coordinate_updates(locodes):\n",
    "    ''' Load from WPI to fill in (some) missing coordinates (https://msi.nga.mil/Publications/WPI World Port Index)'''\n",
    "\n",
    "    WPI_locodes    = pd.read_csv(\"../Map Data/Ports/UpdatedPub150.csv\", usecols=[\"UN/LOCODE\", \"Main Port Name\", \"Latitude\", \"Longitude\"]) \n",
    "    missing_coordinates = locodes[locodes['Coordinates'].isna()].copy()\n",
    "    missing_coordinates[\"UN/LOCODE\"] = missing_coordinates[\"Country\"] + \" \" + missing_coordinates[\"Location\"]\n",
    "    missing_coordinates = missing_coordinates[[\"Country\", \"Location\", \"UN/LOCODE\"]]\n",
    "\n",
    "    # Merging will reset index, turn it into a column to save it\n",
    "    missing_coordinates.reset_index(inplace = True)\n",
    "\n",
    "    missing_coordinates = missing_coordinates.merge(WPI_locodes, on = \"UN/LOCODE\")\n",
    "\n",
    "    # Use the old index again\n",
    "    missing_coordinates.set_index('index', inplace = True)\n",
    "    missing_coordinates.index.name = None\n",
    "\n",
    "    # Some WPI locodes have multiple rows, drop these duplicates\n",
    "    missing_coordinates = missing_coordinates.drop_duplicates(subset = [\"UN/LOCODE\"])\n",
    "\n",
    "    # Split coordinates to floats from string\n",
    "    locodes = extract_coordinates(locodes)\n",
    "\n",
    "    # Fill in missing coordinates from WPI (using index matching)\n",
    "    locodes.update(missing_coordinates[[\"Latitude\", \"Longitude\"]])\n",
    "   \n",
    "    return locodes\n",
    "\n",
    "def add_missing_port_data(locodes):\n",
    "    '''Add missing port data'''\n",
    "    # TODO: Check https://ec.europa.eu/eurostat/cache/metadata/Annexes/mar_esms_an2.xlsx for better data, has some of the missing rows at least?\n",
    "\n",
    "    # Add missing coordinates\n",
    "    locodes = manual_coordinate_updates(locodes)\n",
    "\n",
    "    # Add completely new rows for frequently used missing locations\n",
    "    locodes = manual_port_additions(locodes)\n",
    "\n",
    "    # Load from WPI to fill in (some) missing coordinates (https://msi.nga.mil/Publications/WPI World Port Index)\n",
    "    locodes = world_port_index_coordinate_updates(locodes)\n",
    "\n",
    "    # Drop any rows with still missing coordinates\n",
    "    locodes.dropna(subset=[\"Latitude\",  \"Longitude\"], inplace = True, ignore_index = True)\n",
    "\n",
    "    return locodes\n",
    "\n",
    "def load_port_data():\n",
    "    '''Load UN locode data'''\n",
    "    \n",
    "    na_values = [\"\", \n",
    "             \"#N/A\", \n",
    "             \"#N/A N/A\", \n",
    "             \"#NA\", \n",
    "             \"-1.#IND\", \n",
    "             \"-1.#QNAN\", \n",
    "             \"-NaN\", \n",
    "             \"-nan\", \n",
    "             \"1.#IND\", \n",
    "             \"1.#QNAN\", \n",
    "             \"<NA>\", \n",
    "             \"N/A\", \n",
    "             #\"NA\", # Needed to prevent Namibia (NA) from being interpreted as missing value\n",
    "             \"NULL\", \n",
    "             \"NaN\", \n",
    "             \"n/a\", \n",
    "             \"nan\", \n",
    "             \"null\"]\n",
    "    \n",
    "    # Load The United Nations Code for Trade and Transport Locations\n",
    "    locodes = pd.read_csv(\"../Map Data/Ports/code-list_csv.csv\", \n",
    "                          usecols=[\"Country\", \"Location\", \"Name\", \"NameWoDiacritics\", \"Coordinates\", \"Function\"], \n",
    "                          keep_default_na = False, na_values = na_values) \n",
    "    # NOTE: Currently https://datahub.io/core/un-locode (outdated)\n",
    "    # but https://unece.org/trade/cefact/UNLOCODE-Download is the true source\n",
    "    # 2023 version however split and offers no noticeable improvements\n",
    "\n",
    "    # Limit to ports, canals and inland ports\n",
    "    # 0 = Function not known, to be specified # Just in case\n",
    "    # 1 = Port\n",
    "    # 6 = Multimodal Functions (ICDs, etc.)   # No idea what that means, but includes canals like DECKL - Kiel canal\n",
    "    # 8 = Inland ports                        # Just in case\n",
    "\n",
    "    # TODO: Consider adding some inland locations if they're used enough\n",
    "    # Edit frequently used location's classifications\n",
    "    locodes.loc[(locodes[\"Country\"] == \"EE\") & (locodes[\"Location\"] == \"PLA\"), \"Function\"] = \"123----B\" # Also has 2 separate codes for ports, so we simplify here\n",
    "    locodes.loc[(locodes[\"Country\"] == \"FI\") & (locodes[\"Location\"] == \"RAH\"), \"Function\"] = \"1-3-----\" # Incorrectly only marked 3\n",
    "\n",
    "    locodes = locodes.loc[locodes[\"Function\"].str.contains(\"[0168]\", na = False)]\n",
    "\n",
    "    # Drop port identifier column\n",
    "    locodes = locodes.drop(columns=['Function'])\n",
    "\n",
    "    # Remove troublesome names from name matching\n",
    "    locodes.loc[locodes[\"NameWoDiacritics\"] == \"Russia\", \"NameWoDiacritics\"]  = pd.NA\n",
    "    # locodes.loc[locodes[\"NameWoDiacritics\"] == \"Denmark\", \"NameWoDiacritics\"] = pd.NA # No practical difference\n",
    "    locodes.loc[locodes[\"NameWoDiacritics\"] == \"Rescue\", \"NameWoDiacritics\"]  = pd.NA # \"Search & Rescue\" vs. USRES\n",
    "    locodes.loc[locodes[\"NameWoDiacritics\"] == \"Harbor\", \"NameWoDiacritics\"]  = pd.NA\n",
    "    locodes.loc[locodes[\"NameWoDiacritics\"] == \"Hel\",    \"NameWoDiacritics\"]  = pd.NA\n",
    "    locodes.loc[locodes[\"NameWoDiacritics\"] == \"Baltic\", \"NameWoDiacritics\"]  = pd.NA\n",
    "\n",
    "    # Add missing data\n",
    "    locodes = add_missing_port_data(locodes)\n",
    "    \n",
    "    # Format port names and locodes for regex purposes\n",
    "    locodes = format_port_identifiers(locodes)\n",
    " \n",
    "    return locodes\n",
    "\n",
    "def match_locodes(meta_df, locodes):\n",
    "    '''Use regex to get valid locodes from destination column'''\n",
    "    # Replace destination underscores with spaces for easier matching at word boundaries\n",
    "    meta_df.replace({\"destination\": {\"_\": \" \"}}, regex = True, inplace = True)\n",
    "\n",
    "    # Take only non-NA locodes, use with regex on metadata\n",
    "    valid_patterns = locodes.dropna(subset=[\"LocodePattern\"])[\"LocodePattern\"]\n",
    "\n",
    "    locode_pattern = \"|\".join(valid_patterns)\n",
    "    extracted_locodes = meta_df.destination.str.extractall(r\"\\b(\" + locode_pattern + r\")\\b\")\n",
    "\n",
    "    # Turn multi-index into columns\n",
    "    locode_matches = extracted_locodes.reset_index(level = [\"match\"]).pivot(columns = \"match\")\n",
    "    locode_matches.columns = locode_matches.columns.droplevel()\n",
    "\n",
    "    locode_matches.rename(columns = {0: \"destinationOne\", 1: \"destinationTwo\", 2: \"destinationThree\"}, inplace = True)\n",
    "    locode_matches.columns.name = None\n",
    "\n",
    "    # Ensure number of columns is 3 for consistency\n",
    "    temp = pd.DataFrame(index = locode_matches.index, columns = ['destinationOne', 'destinationTwo', 'destinationThree'], dtype = \"string\")\n",
    "    temp.update(locode_matches.iloc[:, 0:3])\n",
    "    locode_matches = temp\n",
    "\n",
    "    # Remove excess whitespace for easier matching\n",
    "    locode_matches.replace(r\"\\s+\", \" \", inplace = True, regex = True)\n",
    "    locode_matches.destinationOne.str.strip()\n",
    "    locode_matches.destinationTwo.str.strip()\n",
    "    locode_matches.destinationThree.str.strip()\n",
    "\n",
    "    # Merge to metadata\n",
    "    meta_df = meta_df.merge(locode_matches, how = 'left', left_index = True, right_index = True)\n",
    "\n",
    "    return meta_df\n",
    "\n",
    "def replace_invalid_alt_port_names(loc_name_matches, invalid_alt_names, column_name):\n",
    "    '''Replace alternate port names with the ones we use to find locodes'''\n",
    "    # Merging will reset index, turn it into a column to save it\n",
    "    loc_name_matches.reset_index(inplace = True)\n",
    "\n",
    "    loc_name_matches = loc_name_matches.merge(invalid_alt_names[[\"Name2\", \"Name1\"]], how='left', left_on=column_name, right_on=\"Name2\")\n",
    "    # Use the old index again\n",
    "    loc_name_matches.set_index('index', inplace = True)\n",
    "    loc_name_matches.index.name = None\n",
    "\n",
    "    # Replace names not used in locodes\n",
    "    loc_name_matches[column_name] = loc_name_matches[column_name].mask(loc_name_matches['Name2'].notna(), loc_name_matches['Name1'])\n",
    "\n",
    "    loc_name_matches.drop(columns = [\"Name2\", \"Name1\"], inplace = True)\n",
    "\n",
    "    return loc_name_matches\n",
    "\n",
    "def match_port_names(meta_df, locodes):\n",
    "    '''Use regex to get valid port names from destination column'''\n",
    "    # Get all destinations that weren't valid locodes\n",
    "    destination_names = meta_df[meta_df[\"destinationOne\"].isna()].destination\n",
    "\n",
    "    # Get all non-empty name patterns\n",
    "    valid_patterns = locodes.dropna(subset=[\"NamePattern\"])[\"NamePattern\"]\n",
    "\n",
    "    # Use patterns with regex on destinations\n",
    "    loc_name_pattern = \"|\".join(valid_patterns)\n",
    "    extracted_loc_names = destination_names.str.extractall(r\"\\b(\" + loc_name_pattern + r\")\\b\")\n",
    "\n",
    "    # Turn multi-index into columns\n",
    "    loc_name_matches = extracted_loc_names.reset_index(level = [\"match\"]).pivot(columns = \"match\")\n",
    "    loc_name_matches.columns = loc_name_matches.columns.droplevel()\n",
    "\n",
    "    loc_name_matches.rename(columns = {0: \"SimpleNameOne\", 1: \"SimpleNameTwo\", 2: \"SimpleNameThree\"}, inplace = True)\n",
    "    loc_name_matches.columns.name = None\n",
    "\n",
    "    # Ensure number of columns is 3 for consistency\n",
    "    temp = pd.DataFrame(index = loc_name_matches.index, columns = ['SimpleNameOne', 'SimpleNameTwo', 'SimpleNameThree'], dtype = \"string\")\n",
    "    temp.update(loc_name_matches.iloc[:, 0:3])\n",
    "    loc_name_matches = temp\n",
    "\n",
    "    # Manually replace certain names not used in locodes\n",
    "    loc_name_matches.replace(\".*(PETERSBURG|PETERBURG|SPB)\", \"SAINT PETERSBURG\", inplace = True, regex = True) # TODO: Consider LED\n",
    "    loc_name_matches.replace(\"ANTWERP\", \"ANTWERPEN\", inplace = True, regex = True)\n",
    "    loc_name_matches.replace(\"PANSIO\", \"TURKU\", inplace = True, regex = True)\n",
    "    # TODO: Consider HEL, HKI\n",
    "\n",
    "    # Manipulate columns for easier merging\n",
    "    loc_name_matches.replace(r\"[^A-Z\\s]\", \"\", inplace = True, regex = True)\n",
    "    loc_name_matches.replace(r\"\\s+\", \" \", inplace = True, regex = True)\n",
    "    loc_name_matches.SimpleNameOne.str.strip()\n",
    "    loc_name_matches.SimpleNameTwo.str.strip()\n",
    "    loc_name_matches.SimpleNameThree.str.strip()\n",
    "\n",
    "    # Create a dataframe from the placenames with alternatives\n",
    "    multi_patterns = pd.DataFrame(valid_patterns.loc[valid_patterns.str.contains(\"\\|\", na = False)])\n",
    "\n",
    "    multi_patterns[\"Name1\"] = multi_patterns.NamePattern.str.extract(r\"(.*)\\|\")\n",
    "    multi_patterns[\"Name2\"] = multi_patterns.NamePattern.str.extract(r\"\\|(.*)\")\n",
    "\n",
    "    # Choose the ones that aren't used in locodes-dataframe so we can replace them\n",
    "    invalid_alt_names = multi_patterns[~multi_patterns['Name2'].isin(locodes)].copy()\n",
    "\n",
    "    # Edit for easier merging\n",
    "    invalid_alt_names.replace(r\"[^A-Z\\s]\", \"\", inplace = True, regex = True)\n",
    "    invalid_alt_names.replace(r\"\\s+\", \" \", inplace = True, regex = True)\n",
    "    invalid_alt_names.Name1.str.strip()\n",
    "    invalid_alt_names.Name2.str.strip()\n",
    "    invalid_alt_names = invalid_alt_names.drop(columns=[\"NamePattern\"])\n",
    "    invalid_alt_names.drop_duplicates(subset = [\"Name2\"], inplace = True) # If one alt name maps to multiple, just pick first - should be good enough\n",
    "\n",
    "    loc_name_matches.dropna(subset = [\"SimpleNameOne\"], inplace = True)\n",
    "\n",
    "    loc_name_matches = replace_invalid_alt_port_names(loc_name_matches, invalid_alt_names, \"SimpleNameOne\")\n",
    "    loc_name_matches = replace_invalid_alt_port_names(loc_name_matches, invalid_alt_names, \"SimpleNameTwo\")\n",
    "    loc_name_matches = replace_invalid_alt_port_names(loc_name_matches, invalid_alt_names, \"SimpleNameThree\")\n",
    "\n",
    "    return loc_name_matches\n",
    "\n",
    "def extract_locode_from_name(extracted_loc_names, locodes, name_column, destination_column):\n",
    "    '''Use port names to retrieve corresponding locodes'''  \n",
    "    # Make a copy so we can drop NAs for merging\n",
    "    # Merging will reset index, turn it into a column to save it\n",
    "    names_to_locodes = extracted_loc_names.copy().reset_index()[[name_column, 'index']]\n",
    "    names_to_locodes.dropna(inplace = True)\n",
    "\n",
    "    names_to_locodes = names_to_locodes.merge(locodes[[\"SimpleName\", \"Country\", \"Location\"]], how='left', left_on=name_column, right_on=\"SimpleName\")\n",
    "\n",
    "    # Use the old index again\n",
    "    names_to_locodes.set_index('index', inplace = True)\n",
    "    names_to_locodes.index.name = None\n",
    "\n",
    "    names_to_locodes[destination_column] = names_to_locodes[\"Country\"] + names_to_locodes[\"Location\"]\n",
    "    names_to_locodes = names_to_locodes[destination_column]\n",
    "\n",
    "    # Set destinationOne column and merge\n",
    "    extracted_loc_names[destination_column] = pd.NA\n",
    "\n",
    "    extracted_loc_names.update(names_to_locodes)\n",
    "\n",
    "    return extracted_loc_names\n",
    "\n",
    "def match_port_identifiers(meta_df, locodes):\n",
    "    '''Match metadata destinations with port locodes'''    \n",
    "    meta_df = match_locodes(meta_df, locodes)\n",
    "\n",
    "    extracted_loc_names = match_port_names(meta_df, locodes)\n",
    "\n",
    "    # Merge to get locodes\n",
    "    locodes.replace({\"SimpleName\": {r\"[^A-Z\\s]\": \"\"}}, inplace = True, regex = True)\n",
    "    locodes.replace({\"SimpleName\": {r\"\\s+\": \" \"}}, inplace = True, regex = True)\n",
    "    locodes.SimpleName.str.strip()\n",
    "    \n",
    "    extracted_loc_names = extract_locode_from_name(extracted_loc_names, locodes, \"SimpleNameOne\",   \"destinationOne\")\n",
    "    extracted_loc_names = extract_locode_from_name(extracted_loc_names, locodes, \"SimpleNameTwo\",   \"destinationTwo\")\n",
    "    extracted_loc_names = extract_locode_from_name(extracted_loc_names, locodes, \"SimpleNameThree\", \"destinationThree\")\n",
    "\n",
    "    meta_df.update(extracted_loc_names[[\"destinationOne\", \"destinationTwo\", \"destinationThree\"]], overwrite=False)\n",
    "\n",
    "    # For consistency, replace NaNs etc. with NAs\n",
    "    meta_df.fillna(pd.NA, inplace = True)\n",
    "\n",
    "    # Remove all whitespace from destination locodes for easier matching\n",
    "    meta_df.destinationOne = meta_df.destinationOne.str.replace(r\"\\s+\", \"\", regex = True)\n",
    "    meta_df.destinationTwo = meta_df.destinationTwo.str.replace(r\"\\s+\", \"\", regex = True)\n",
    "    meta_df.destinationThree = meta_df.destinationThree.str.replace(r\"\\s+\", \"\", regex = True)\n",
    "\n",
    "    return meta_df\n",
    "\n",
    "def classify_regions(dataframe, latitude_column, longitude_column, region_column):\n",
    "    '''Classify locations based on coordinates''' \n",
    "    # NOTE: GeoPandas uses LonLat\n",
    "    bothnian_bay    = Polygon([[22,  66],  [25.6,65.9],[26,  65],  [22.5,63.1],[19.7,63.6]])\n",
    "    bothnian_sea    = Polygon([[16.6,63],  [16.6,60.5],[18,  60.5],[21.5,60.7],[22.5,63.1],[19.7,63.6]])\n",
    "    archipelago_sea = Polygon([[18,  60.5],[21.5,60.7],[23,  60.5],[23.5,60.3],[21.8,59.4],[18.6,59.7]])\n",
    "    gulf_of_finland = Polygon([[23.5,60.3],[21.8,59.4],[23.5,58.8],[30.8,59.5],[29.5,61]])\n",
    "    saimaa_laatokka = Polygon([[30.8,59.5],[29.5,61],  [23.5,60.3],[25,  64],  [31,  64],  [34,  60]])\n",
    "\n",
    "    locations_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(dataframe[longitude_column], dataframe[latitude_column]), crs=\"EPSG:4326\")\n",
    "\n",
    "    dataframe[region_column] = \"Baltic Sea\"\n",
    "\n",
    "    dataframe.loc[locations_gdf.intersects(bothnian_bay),    region_column] = \"Bothnian Bay\"\n",
    "    dataframe.loc[locations_gdf.intersects(bothnian_sea),    region_column] = \"Bothnian Sea\"\n",
    "    dataframe.loc[locations_gdf.intersects(archipelago_sea), region_column] = \"Archipelago Sea\"\n",
    "    dataframe.loc[locations_gdf.intersects(gulf_of_finland), region_column] = \"Gulf of Finland\"\n",
    "    dataframe.loc[locations_gdf.intersects(saimaa_laatokka), region_column] = \"Saimaa and Laatokka\"\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def classify_destination_column(meta_df, regions, destination_column, region_column):\n",
    "    '''Classify a column of destination locations relative to archipelago'''\n",
    "    meta_df = meta_df.merge(regions, how='left', left_on=destination_column, right_on=\"Locode\")\n",
    "    meta_df.rename(columns = {\"PortLocation\": region_column}, inplace = True)\n",
    "    meta_df.drop(columns=[\"Locode\"], inplace = True)\n",
    "\n",
    "    return meta_df\n",
    "\n",
    "def classify_destination_regions(meta_df, regions):\n",
    "    '''Classify destination locations relative to archipelago''' \n",
    "    regions[\"Locode\"] = regions[\"Country\"] + regions[\"Location\"]\n",
    "    regions.drop(columns=[\"Country\", \"Location\"], inplace = True)\n",
    "    regions.drop_duplicates(inplace = True)\n",
    "\n",
    "    meta_df = classify_destination_column(meta_df, regions, \"destinationOne\",   \"destinationOneRegion\")\n",
    "    meta_df = classify_destination_column(meta_df, regions, \"destinationTwo\",   \"destinationTwoRegion\")\n",
    "    meta_df = classify_destination_column(meta_df, regions, \"destinationThree\", \"destinationThreeRegion\")\n",
    "\n",
    "    return meta_df\n",
    "\n",
    "def analyze_destinations(meta_df):\n",
    "    '''Parse and edit destinations from ship metadata'''\n",
    "    locodes = load_port_data()\n",
    "    meta_df = match_port_identifiers(meta_df, locodes) # NOTE: Contains in-place editing of locodes-dataframe\n",
    "\n",
    "    locodes = classify_regions(locodes, \"Latitude\", \"Longitude\", \"PortLocation\")\n",
    "    meta_df = classify_destination_regions(meta_df, locodes[[\"Country\", \"Location\", \"PortLocation\"]].copy())\n",
    "\n",
    "    return meta_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_year(dt):\n",
    "        '''Set datetime one year ahead'''\n",
    "        \n",
    "        try:\n",
    "           return dt.replace(year=dt.year+1)\n",
    "        except ValueError:\n",
    "           # February 29th in a leap year\n",
    "           # Add 365 days instead to arrive at March 1st\n",
    "           return dt + timedelta(days=365)\n",
    "\n",
    "def eta_to_datetime(eta):\n",
    "    '''Convert digitraffic AIS ETAs to datetime\n",
    "    \n",
    "       Digitraffic stores ETA as an integer (e.g. 557760) \n",
    "       that when converted to binary encodes the date and time as such:\n",
    "       \n",
    "       Bits 19-16:  month; 1-12;  0 = not available = default \n",
    "       Bits 15-11:    day; 1-31;  0 = not available = default \n",
    "       Bits  10-6:   hour; 0-23; 24 = not available = default \n",
    "       Bits   5-0: minute; 0-59; 60 = not available = default'''\n",
    "\n",
    "    if(eta == 1596): # All default / not available (00/00 24:60 MM/DD HH:MM)\n",
    "        return None\n",
    "\n",
    "    eta_bin = format(eta, '0b').zfill(20)\n",
    "    \n",
    "    eta_min = int(eta_bin[ -6:   ], 2) % 60 # Set default/NA to 0 for datetime conversion\n",
    "    eta_hr  = int(eta_bin[-11: -6], 2) % 24 # Set default/NA to 0 for datetime conversion\n",
    "    eta_day = int(eta_bin[-16:-11], 2)\n",
    "    eta_mth = int(eta_bin[-20:-16], 2)\n",
    "\n",
    "    # Probably unnecessary but better safe than sorry:\n",
    "    if(eta_mth == 0 or eta_day == 0): # Consider perhaps defaulting to today/tomorrow, but that seems risky\n",
    "        return None\n",
    "\n",
    "    # Check if ETA STILL bad (e.g. 31st September...)\n",
    "    try:\n",
    "        eta_datetime = datetime(datetime.now().year, eta_mth, eta_day, eta_hr, eta_min)\n",
    "    except:\n",
    "        print(\"Bad ETA: \", eta, \" = \", datetime.now().year, \"/\", eta_mth, \"/\", eta_day, eta_hr, \":\", eta_min)\n",
    "        return None\n",
    "\n",
    "    # Simple check if ETA is e.g. from this year's December to next year's January\n",
    "    if(eta_datetime < datetime.now() - timedelta(days=180)):\n",
    "        eta_datetime = next_year(eta_datetime)\n",
    "\n",
    "    return eta_datetime\n",
    "\n",
    "def collect_ships_locations(latitude, longitude, distance, since):\n",
    "    '''Collect ships' location data into a dataframe from given location, distance and time'''\n",
    "\n",
    "    # NOTE: Python timestamps in seconds, digitraffic in milliseconds\n",
    "    current_timestamp = floor(datetime.now().timestamp()*1000)\n",
    "    \n",
    "    ship_collection = get_ships_locations(latitude, longitude, distance, since)\n",
    "    ships = ship_collection[\"features\"]\n",
    "\n",
    "    ''' Example:\n",
    "    ... 'features': ...\n",
    "    'geometry': {'type': 'Point', 'coordinates': [22.949432, 59.821617]},\n",
    "    'properties': {'mmsi': 209955000,\n",
    "    'sog': 0.0,\n",
    "    'cog': 57.2,\n",
    "    'navStat': 5,\n",
    "    'rot': 0,\n",
    "    'posAcc': False,\n",
    "    'raim': False,\n",
    "    'heading': 258,\n",
    "    'timestamp': 43,\n",
    "    'timestampExternal': 1692345778776}'''\n",
    "    \n",
    "    ship_dict = [dict(ship['properties'], \n",
    "                     **{'longitude':ship['geometry']['coordinates'][0], \n",
    "                        'latitude' :ship['geometry']['coordinates'][1]}) \n",
    "                for ship in ships]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(ship_dict)\n",
    "    df['locAPICallTimestamp'] = current_timestamp\n",
    "\n",
    "    # Remove unnecessary columns:\n",
    "    # Receiver autonomous integrity monitoring (RAIM) flag of electronic position fixing device\n",
    "    # The second within the minute data was reported\n",
    "\n",
    "    df = df.drop([\"raim\", \"timestamp\"], axis=1)\n",
    "    \n",
    "    # Rename timestamp column\n",
    "    df = df.rename(columns={\"timestampExternal\": \"locUpdateTimestamp\"}) # Descriptive name for merging tables later\n",
    "\n",
    "    # Replace default / not available values with NAs\n",
    "    df = df.replace({'sog': 102.3, \n",
    "                     'cog': 360, \n",
    "                     'rot': -128, \n",
    "                     'heading': 511}, np.nan)\n",
    "    \n",
    "    df = classify_regions(df, \"latitude\", \"longitude\", \"shipRegion\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def collect_ships_meta(since):\n",
    "    '''Collect ships' metadata into a dataframe since given time'''\n",
    "\n",
    "    # NOTE: Python timestamps in seconds, digitraffic in milliseconds\n",
    "    current_timestamp = floor(datetime.now().timestamp()*1000)\n",
    "    \n",
    "    ships = get_ships_meta(since)\n",
    "\n",
    "    ''' Example:\n",
    "    {'name': 'JOHANNA HELENA',\n",
    "    'timestamp': 1692414605620,\n",
    "    'mmsi': 209955000,\n",
    "    'callSign': '5BMF5',\n",
    "    'imo': 9372212,\n",
    "    'shipType': 70,\n",
    "    'draught': 55,\n",
    "    'eta': 563840,\n",
    "    'posType': 1,\n",
    "    'referencePointA': 96,\n",
    "    'referencePointB': 19,\n",
    "    'referencePointC': 8,\n",
    "    'referencePointD': 8,\n",
    "    'destination': 'SE OXE'}'''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(ships)\n",
    "    df['metaAPICallTimestamp'] = current_timestamp\n",
    "\n",
    "    # Remove unnecessary columns:\n",
    "    # Vessel International Maritime Organization (IMO) number\n",
    "    # Type of electronic position fixing device (GPS, GLONASS, etc.)\n",
    "    # GNSS antenna position reference\n",
    "\n",
    "    # NOTE: MMSI changes with e.g. nationality, whereas IMO is static. We use MMSI because so does digitraffic\n",
    "\n",
    "    df = df.drop([\"imo\", \"posType\",\n",
    "                  \"referencePointA\", \"referencePointB\",\n",
    "                  \"referencePointC\", \"referencePointD\"], axis=1)\n",
    "\n",
    "    # Replace default / not available values with NAs\n",
    "    df = df.replace({'draught': 0}, np.nan)\n",
    "    \n",
    "    # Rename timestamp column\n",
    "    df = df.rename(columns={\"timestamp\": \"metaUpdateTimestamp\"}) # Descriptive name for merging tables later\n",
    "\n",
    "    # Fix formatting of ETA from integer to datetime\n",
    "    cutoff = datetime.now() - timedelta(days=30)\n",
    "    cutoff_timestamp = floor(cutoff.timestamp()*1000)\n",
    "    df.loc[(df[\"metaUpdateTimestamp\"] < cutoff_timestamp), \"eta\"] = 1596 # Set ETA for metadata updated over a month ago to NA\n",
    "    df[\"eta\"] = df[\"eta\"].apply(eta_to_datetime)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    '''Create a database connection to a SQLite database'''\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def append_table(db_connection, dataframe, table):\n",
    "    '''Append a dataframe to a database table'''\n",
    "\n",
    "    if db_connection is not None:\n",
    "        dataframe.to_sql(table, db_connection, if_exists=\"append\", index=False)\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "\n",
    "def drop_table(db_connection, table):\n",
    "    '''Drop a table from a database'''\n",
    "    # Mainly meant to be a development tool\n",
    "    cursor = db_connection.cursor()\n",
    "\n",
    "    cursor.execute(f'DROP TABLE {table}')\n",
    "    db_connection.commit()\n",
    "\n",
    "def delete_duplicate_rows(db_connection, table, columns):\n",
    "    '''Delete duplicate rows from a database table'''\n",
    "\n",
    "    sql_command = f'DELETE FROM {table} WHERE rowid NOT IN (SELECT MIN(rowid) FROM {table} GROUP BY {columns})'\n",
    "    sql_cursor = db_connection.cursor()\n",
    "    sql_cursor.execute(sql_command)\n",
    "    db_connection.commit()\n",
    "\n",
    "def delete_old(db_connection, table, column, timestamp):\n",
    "    '''Delete rows from a database table with updatetime < given time'''\n",
    "\n",
    "    sql_command = f'DELETE FROM {table} WHERE {column} < {timestamp}'\n",
    "    sql_cursor = db_connection.cursor()\n",
    "    sql_cursor.execute(sql_command)\n",
    "    db_connection.commit()\n",
    "\n",
    "def update_meta_table(db_connection, since): # TODO: In-depth QA\n",
    "    '''Update database meta table with with data since last update'''\n",
    "    meta_df = collect_ships_meta(since)\n",
    "    meta_df = analyze_destinations(meta_df)\n",
    "\n",
    "    # Simply create a table if one doesn't exist\n",
    "    try:\n",
    "        meta_df.to_sql(\"meta\", db_connection, if_exists=\"fail\", index=False)\n",
    "        return\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Create a temp table from the dataframe\n",
    "    meta_df.to_sql(\"temp\", db_connection, if_exists=\"replace\", index=False)\n",
    "\n",
    "    columns = list(meta_df.columns)\n",
    "\n",
    "    # Insert whole row where the mmsi in dataframe isn't in table\n",
    "    columns_string = \", \".join(columns)\n",
    "    sql_insert_query = f'INSERT INTO meta ({columns_string}) SELECT {columns_string} FROM temp AS t '\n",
    "    sql_insert_query += \"WHERE NOT EXISTS (SELECT mmsi FROM meta AS sub WHERE sub.mmsi = t.mmsi);\"\n",
    "\n",
    "    # Update non-mmsi columns where mmsi in both dataframe and table\n",
    "    columns.remove(\"mmsi\")\n",
    "    set_list = []\n",
    "    for column in columns:\n",
    "        set_list.append(f'{column} = temp.{column}')\n",
    "    set_string = \", \".join(set_list)\n",
    "\n",
    "    sql_set_query = f\"UPDATE meta SET {set_string} FROM temp WHERE temp.mmsi = meta.mmsi;\"\n",
    "\n",
    "    # Perform queries\n",
    "    cursor = db_connection.cursor()\n",
    "\n",
    "    cursor.execute(sql_set_query)\n",
    "    db_connection.commit()\n",
    "\n",
    "    cursor.execute(sql_insert_query)\n",
    "    db_connection.commit()\n",
    "\n",
    "    # Remove the temp table\n",
    "    cursor.execute(\"DROP TABLE temp\")\n",
    "    db_connection.commit()\n",
    "\n",
    "def get_latest_meta_update_timestamp(db_connection):\n",
    "    '''Get the latest update timestamp from meta table'''\n",
    "    query = (\"SELECT MAX(metaUpdateTimestamp) from meta\")\n",
    "\n",
    "    cursor = db_connection.cursor()\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    timestamp = cursor.fetchone()[0]\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# TODO: Choose proper values\n",
    "latitude = 60\n",
    "longitude = 20\n",
    "distance = 700\n",
    "\n",
    "# NOTE: Python timestamps in seconds, digitraffic in milliseconds\n",
    "yesterday = datetime.now() - timedelta(hours=24, minutes=0)\n",
    "since = floor(yesterday.timestamp()*1000)\n",
    "\n",
    "\n",
    "database = \"AIS.sqlite\"\n",
    "db_connection = create_connection(database)\n",
    "\n",
    "# # NOTE: Only initialize once when creating database. Second update_meta_table unnecessary when initializing.\n",
    "# #       Initializing more than once should not cause errors, but is a waste of resources\n",
    "# initialization_timestamp = floor(datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\").timestamp()*1000)\n",
    "# update_meta_table(db_connection, initialization_timestamp)\n",
    "\n",
    "# TODO: Only update meta as necessary - requires calling db to check? Perhaps do check in Draw_Map then call for update here?\n",
    "update_meta_table(db_connection, get_latest_meta_update_timestamp(db_connection))\n",
    "\n",
    "# # NOTE: Instead of updating existing metadata, you can also simply append by using\n",
    "# meta_df = collect_ships_meta(since)\n",
    "# append_table(db_connection, meta_df, \"meta\")\n",
    "# delete_duplicate_rows(db_connection, \"meta\", \", \".join(list(meta_df.columns)))\n",
    "# # This however gets ALL ships that changed metadata - using locations -> mmsi probably shouldn't be used due to being very slow (>0.5s per mmsi)\n",
    "# # TODO: Consider deleting rows where mmsi not in locations table so we only store data from within coordinates\n",
    "\n",
    "# TODO: Update locations regularly - using CRON?\n",
    "locations_df = collect_ships_locations(latitude, longitude, distance, since)\n",
    "append_table(db_connection, locations_df, \"locations\")\n",
    "delete_duplicate_rows(db_connection, \"locations\", \", \".join(list(locations_df.columns))) # TODO: Detect and fix outliers\n",
    "\n",
    "\n",
    "# TODO: Set reasonable cutoff\n",
    "time_cutoff_dt = datetime.now() - timedelta(days=30)\n",
    "time_cutoff_ts = floor(time_cutoff_dt.timestamp()*1000)\n",
    "delete_old(db_connection, \"locations\", \"locUpdateTimestamp\", time_cutoff_ts)\n",
    "\n",
    "db_connection.close() \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose proper values\n",
    "latitude = 60\n",
    "longitude = 20\n",
    "distance = 700\n",
    "\n",
    "# NOTE: Python timestamps in seconds, digitraffic in milliseconds\n",
    "yesterday = datetime.now() - timedelta(hours=24, minutes=0)\n",
    "since = floor(yesterday.timestamp()*1000)\n",
    "\n",
    "database = \"../Map Data/test.sqlite\"\n",
    "db_connection = create_connection(database)\n",
    "\n",
    "# # NOTE: Only initialize once when creating database. Second update_meta_table unnecessary when initializing.\n",
    "# #       Initializing more than once should not cause errors, but is a waste of resources\n",
    "# initialization_timestamp = floor(datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\").timestamp()*1000)\n",
    "# update_meta_table(db_connection, initialization_timestamp)\n",
    "\n",
    "# TODO: Only update meta as necessary - requires calling db to check? Perhaps do check in Draw_Map then call for update here?\n",
    "update_meta_table(db_connection, get_latest_meta_update_timestamp(db_connection))\n",
    "\n",
    "# # NOTE: Instead of updating existing metadata, you can also simply append by using\n",
    "# meta_df = collect_ships_meta(since)\n",
    "# append_table(db_connection, meta_df, \"meta\")\n",
    "# delete_duplicate_rows(db_connection, \"meta\", \", \".join(list(meta_df.columns)))\n",
    "# # NOTE: This however gets ALL ships that changed metadata - using locations -> mmsi probably shouldn't be used due to being very slow (>0.5s per mmsi)\n",
    "# # TODO: Consider deleting rows where mmsi not in locations table so we only store data from within coordinates\n",
    "\n",
    "# TODO: Update locations regularly - using CRON?\n",
    "locations_df = collect_ships_locations(latitude, longitude, distance, since)\n",
    "append_table(db_connection, locations_df, \"locations\")\n",
    "delete_duplicate_rows(db_connection, \"locations\", \", \".join(list(locations_df.columns))) # TODO: Detect and fix outliers\n",
    "\n",
    "\n",
    "# TODO: Set reasonable cutoff\n",
    "time_cutoff_dt = datetime.now() - timedelta(days=30)\n",
    "time_cutoff_ts = floor(time_cutoff_dt.timestamp()*1000)\n",
    "delete_old(db_connection, \"locations\", \"locUpdateTimestamp\", time_cutoff_ts)\n",
    "\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose proper values\n",
    "latitude = 60\n",
    "longitude = 20\n",
    "distance = 700\n",
    "\n",
    "# NOTE: Python timestamps in seconds, digitraffic in milliseconds\n",
    "yesterday = datetime.now() - timedelta(hours=24, minutes=0)\n",
    "since = floor(yesterday.timestamp()*1000)\n",
    "\n",
    "\n",
    "database = \"../Map Data/test.sqlite\"\n",
    "db_connection = create_connection(database)\n",
    "\n",
    "# # NOTE: Only initialize once when creating database. Second update_meta_table unnecessary when initializing.\n",
    "# #       Initializing more than once should not cause errors, but is a waste of resources\n",
    "initialization_timestamp = floor(datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\").timestamp()*1000)\n",
    "update_meta_table(db_connection, initialization_timestamp)\n",
    "\n",
    "# TODO: Only update meta as necessary - requires calling db to check? Perhaps do check in Draw_Map then call for update here?\n",
    "# update_meta_table(db_connection, get_latest_meta_update_timestamp(db_connection))\n",
    "\n",
    "# # NOTE: Instead of updating existing metadata, you can also simply append by using\n",
    "# meta_df = collect_ships_meta(since)\n",
    "# append_table(db_connection, meta_df, \"meta\")\n",
    "# delete_duplicate_rows(db_connection, \"meta\", \", \".join(list(meta_df.columns)))\n",
    "# # NOTE: This however gets ALL ships that changed metadata - using locations -> mmsi probably shouldn't be used due to being very slow (>0.5s per mmsi)\n",
    "# # TODO: Consider deleting rows where mmsi not in locations table so we only store data from within coordinates\n",
    "\n",
    "# TODO: Update locations regularly - using CRON?\n",
    "locations_df = collect_ships_locations(latitude, longitude, distance, since)\n",
    "append_table(db_connection, locations_df, \"locations\")\n",
    "delete_duplicate_rows(db_connection, \"locations\", \", \".join(list(locations_df.columns))) # TODO: Detect and fix outliers\n",
    "\n",
    "\n",
    "# TODO: Set reasonable cutoff\n",
    "time_cutoff_dt = datetime.now() - timedelta(days=30)\n",
    "time_cutoff_ts = floor(time_cutoff_dt.timestamp()*1000)\n",
    "delete_old(db_connection, \"locations\", \"locUpdateTimestamp\", time_cutoff_ts)\n",
    "\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
